{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "289960c0",
   "metadata": {},
   "source": [
    "# Inkswarm DetectLab — Step Runner Notebook (D-0022)\n",
    "\n",
    "This notebook lets you execute the pipeline **step-by-step** for **`login_attempt`**, while reusing the existing code structure.\n",
    "\n",
    "## Steps (run in order, or selectively)\n",
    "1. **Raw + Dataset** (or reuse)\n",
    "2. **Features** (or reuse / shared-cache restore)\n",
    "3. **Baselines** (or reuse artifacts)\n",
    "4. **Eval** (slice + stability reports)\n",
    "5. **Export** (summary, UI bundle, handover, evidence)\n",
    "\n",
    "## Deep dives (optional)\n",
    "- `02_featurelab_login_attempt.ipynb` — feature exploration\n",
    "- `03_baselinelab_login_attempt.ipynb` — baseline exploration\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e9e9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_ROOT: C:\\Users\\Martín\\Desktop\\inkswarm-core\\usul-inkswarm-detectlab\n",
      "CONFIGS_DIR: C:\\Users\\Martín\\Desktop\\inkswarm-core\\usul-inkswarm-detectlab\\configs\n",
      "CFG_PATH: C:\\Users\\Martín\\Desktop\\inkswarm-core\\usul-inkswarm-detectlab\\configs\\skynet_smoke.yaml\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import inkswarm_detectlab\n",
    "\n",
    "# Robust repo root detection (works regardless of Jupyter CWD)\n",
    "pkg_file = Path(inkswarm_detectlab.__file__).resolve()  # .../src/inkswarm_detectlab/__init__.py\n",
    "repo_root = pkg_file.parents[2]\n",
    "if repo_root.name == 'src':\n",
    "    repo_root = repo_root.parent\n",
    "\n",
    "# Locate configs dir\n",
    "configs_dir = repo_root / 'configs'\n",
    "if not configs_dir.exists():\n",
    "    # Fallback: walk upwards looking for a configs/ folder\n",
    "    cand = repo_root\n",
    "    while cand != cand.parent and not (cand / 'configs').exists():\n",
    "        cand = cand.parent\n",
    "    configs_dir = cand / 'configs'\n",
    "\n",
    "# Choose config file\n",
    "cfg_basename = os.environ.get('INKSWARM_CFG', 'skynet_smoke.yaml')\n",
    "matches = list(configs_dir.rglob(cfg_basename)) if configs_dir.exists() else []\n",
    "if not matches:\n",
    "    # Fallback: pick any yaml in configs_dir\n",
    "    matches = sorted(configs_dir.rglob('*.yaml')) if configs_dir.exists() else []\n",
    "if not matches:\n",
    "    raise FileNotFoundError(f'Could not find config under configs_dir={configs_dir} (repo_root={repo_root})')\n",
    "\n",
    "CFG_PATH = matches[0]\n",
    "print('REPO_ROOT:', repo_root)\n",
    "print('CONFIGS_DIR:', configs_dir)\n",
    "print('CFG_PATH:', CFG_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97a4c2f",
   "metadata": {
    "name": "Imports / helpers",
    "tags": [
     "rr"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "# Core imports for the step notebook (kept in one place for restart safety)\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from inkswarm_detectlab.config import load_config\n",
    "from inkswarm_detectlab.ui.steps import StepRecorder\n",
    "from inkswarm_detectlab.ui.notebook_tools import print_run_tree, tail_text\n",
    "from inkswarm_detectlab.ui.step_runner import (\n",
    "    resolve_run_id,\n",
    "    wire_check,\n",
    "    step_dataset,\n",
    "    step_features,\n",
    "    step_baselines,\n",
    "    step_eval,\n",
    "    step_export,\n",
    ")\n",
    "\n",
    "print(\"Imports OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "528322d2",
   "metadata": {
    "name": "User inputs / toggles (defaults)",
    "tags": [
     "rr"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toggles ready: RUN_ID= None REUSE_IF_EXISTS= True\n"
     ]
    }
   ],
   "source": [
    "# DEFAULT TOGGLES (restart-safe)\n",
    "# This notebook is designed to be runnable top-down OR resumed mid-way after restart.\n",
    "# If you run a later cell directly, these defaults prevent NameError for toggles.\n",
    "#\n",
    "# You can override any of these variables in your own cell above the steps.\n",
    "\n",
    "# Run id: set to an existing run to maximize reuse\n",
    "RUN_ID = globals().get(\"RUN_ID\", None)  # e.g. \"RR2_MVP_GIT_B_0002\"\n",
    "\n",
    "# Reuse policy\n",
    "REUSE_IF_EXISTS = globals().get(\"REUSE_IF_EXISTS\", True)\n",
    "\n",
    "# Step toggles\n",
    "DO_STEP_1_DATASET  = globals().get(\"DO_STEP_1_DATASET\", True)\n",
    "FORCE_STEP_1_DATASET = globals().get(\"FORCE_STEP_1_DATASET\", False)\n",
    "\n",
    "DO_STEP_2_FEATURES = globals().get(\"DO_STEP_2_FEATURES\", True)\n",
    "FORCE_STEP_2_FEATURES = globals().get(\"FORCE_STEP_2_FEATURES\", False)\n",
    "USE_SHARED_FEATURE_CACHE = globals().get(\"USE_SHARED_FEATURE_CACHE\", True)\n",
    "WRITE_SHARED_FEATURE_CACHE = globals().get(\"WRITE_SHARED_FEATURE_CACHE\", True)\n",
    "\n",
    "DO_STEP_3_BASELINES = globals().get(\"DO_STEP_3_BASELINES\", True)\n",
    "FORCE_STEP_3_BASELINES = globals().get(\"FORCE_STEP_3_BASELINES\", False)\n",
    "\n",
    "DO_STEP_4_EVAL = globals().get(\"DO_STEP_4_EVAL\", True)\n",
    "FORCE_STEP_4_EVAL = globals().get(\"FORCE_STEP_4_EVAL\", False)\n",
    "\n",
    "DO_STEP_5_EXPORT = globals().get(\"DO_STEP_5_EXPORT\", True)\n",
    "FORCE_STEP_5_EXPORT = globals().get(\"FORCE_STEP_5_EXPORT\", False)\n",
    "\n",
    "print(\"Toggles ready:\",\n",
    "      \"RUN_ID=\", RUN_ID,\n",
    "      \"REUSE_IF_EXISTS=\", REUSE_IF_EXISTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2252444",
   "metadata": {
    "name": "Pre-flight checks (self-heal imports)",
    "tags": [
     "rr"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-flight OK: run_id= RUN_SAMPLE_SMOKE_0001 cfg_path= C:\\Users\\Martín\\Desktop\\inkswarm-core\\usul-inkswarm-detectlab\\configs\\skynet_smoke.yaml\n"
     ]
    }
   ],
   "source": [
    "# PRE-FLIGHT (restart-safe): ensure key symbols exist\n",
    "# This cell prevents NameError when running cells out-of-order or after a kernel restart.\n",
    "\n",
    "missing = []\n",
    "\n",
    "# Ensure core step functions are importable\n",
    "try:\n",
    "    from inkswarm_detectlab.ui.step_runner import (\n",
    "        resolve_run_id,\n",
    "        wire_check,\n",
    "        step_dataset,\n",
    "        step_features,\n",
    "        step_baselines,\n",
    "        step_eval,\n",
    "        step_export,\n",
    "    )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed importing step_runner helpers: {e}\") from e\n",
    "\n",
    "# Ensure utilities are importable\n",
    "try:\n",
    "    from inkswarm_detectlab.ui.steps import StepRecorder\n",
    "    from inkswarm_detectlab.ui.notebook_tools import print_run_tree, tail_text\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed importing notebook utilities: {e}\") from e\n",
    "\n",
    "# Ensure cfg/run_id are initialized\n",
    "from inkswarm_detectlab.config import load_config\n",
    "if \"cfg\" not in globals():\n",
    "    cfg = load_config(CFG_PATH)\n",
    "\n",
    "if \"run_id\" not in globals():\n",
    "    _rid = globals().get(\"RUN_ID\", None)\n",
    "    cfg, run_id = resolve_run_id(CFG_PATH, run_id=_rid)\n",
    "\n",
    "print(\"Pre-flight OK:\",\n",
    "      \"run_id=\", run_id,\n",
    "      \"cfg_path=\", CFG_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf57e154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"run_id\": \"RUN_SAMPLE_SMOKE_0001\",\n",
      "  \"paths\": {\n",
      "    \"run_dir\": \"runs\\\\RUN_SAMPLE_SMOKE_0001\",\n",
      "    \"raw_login_attempt\": \"runs\\\\RUN_SAMPLE_SMOKE_0001\\\\raw\\\\login_attempt.parquet\",\n",
      "    \"raw_checkout_attempt\": \"runs\\\\RUN_SAMPLE_SMOKE_0001\\\\raw\\\\checkout_attempt.parquet\",\n",
      "    \"dataset_train\": \"runs\\\\RUN_SAMPLE_SMOKE_0001\\\\dataset\\\\login_attempt\\\\train.parquet\",\n",
      "    \"dataset_time_eval\": \"runs\\\\RUN_SAMPLE_SMOKE_0001\\\\dataset\\\\login_attempt\\\\time_eval.parquet\",\n",
      "    \"dataset_user_holdout\": \"runs\\\\RUN_SAMPLE_SMOKE_0001\\\\dataset\\\\login_attempt\\\\user_holdout.parquet\",\n",
      "    \"features_login\": \"runs\\\\RUN_SAMPLE_SMOKE_0001\\\\features\\\\login_attempt\\\\features.parquet\",\n",
      "    \"baselines_dir\": \"runs\\\\RUN_SAMPLE_SMOKE_0001\\\\models\\\\login_attempt\\\\baselines\",\n",
      "    \"eval_reports_dir\": \"runs\\\\RUN_SAMPLE_SMOKE_0001\\\\reports\",\n",
      "    \"share_dir\": \"runs\\\\RUN_SAMPLE_SMOKE_0001\\\\share\",\n",
      "    \"manifest\": \"runs\\\\RUN_SAMPLE_SMOKE_0001\\\\manifest.json\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Run tree (quick):\n",
      "- share/logs: runs\\RUN_SAMPLE_SMOKE_0001\\share\\logs (missing)\n",
      "- share/reports: runs\\RUN_SAMPLE_SMOKE_0001\\share\\reports \n",
      "- models: runs\\RUN_SAMPLE_SMOKE_0001\\models \n",
      "- reports: runs\\RUN_SAMPLE_SMOKE_0001\\reports \n",
      "- logs: runs\\RUN_SAMPLE_SMOKE_0001\\logs \n"
     ]
    }
   ],
   "source": [
    "# Step 0 — wiring / paths check (Dry-run A)\n",
    "# Self-contained: imports + guards so it works after a fresh kernel restart.\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from inkswarm_detectlab.config import load_config\n",
    "from inkswarm_detectlab.ui.step_runner import wire_check, resolve_run_id\n",
    "\n",
    "# Ensure cfg + run_id exist\n",
    "if \"cfg\" not in globals():\n",
    "    cfg = load_config(CFG_PATH)\n",
    "\n",
    "if \"run_id\" not in globals():\n",
    "    # Try to honor user-provided RUN_ID if present\n",
    "    _rid = globals().get(\"RUN_ID\", None)\n",
    "    cfg, run_id = resolve_run_id(CFG_PATH, run_id=_rid)\n",
    "else:\n",
    "    # keep existing run_id variable\n",
    "    pass\n",
    "\n",
    "check = wire_check(CFG_PATH, run_id=run_id)\n",
    "print(json.dumps(check, indent=2))\n",
    "rdir = Path(check[\"paths\"][\"run_dir\"])\n",
    "\n",
    "print(\"\\nRun tree (quick):\")\n",
    "from inkswarm_detectlab.ui.notebook_tools import print_run_tree\n",
    "print_run_tree(rdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe894ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility for consistent step prints\n",
    "def _print_outcome(out):\n",
    "    print(f\"\\n=== STEP: {out.name} ===\")\n",
    "    print(\"status:\", out.status)\n",
    "    print(\"decision:\", f\"{out.decision.mode} — {out.decision.reason}\")\n",
    "    print(\"decision_meta:\", f\"used_manifest={out.decision.used_manifest} forced={out.decision.forced}\")\n",
    "    if out.notes:\n",
    "        print(\"notes:\")\n",
    "        for n in out.notes:\n",
    "            print(\" -\", n)\n",
    "    if out.outputs:\n",
    "        print(\"outputs:\")\n",
    "        for k, a in out.outputs.items():\n",
    "            try:\n",
    "                p = a.get(\"path\")  # if dict-like\n",
    "                ex = a.get(\"exists\")\n",
    "                print(f\" - {k}: {p} (exists={ex})\")\n",
    "            except Exception:\n",
    "                print(f\" - {k}: {getattr(a, 'path', '')} (exists={getattr(a, 'exists', None)})\")\n",
    "    if out.summary:\n",
    "        print(\"summary:\", out.summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4a1541e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP: dataset ===\n",
      "status: skipped\n",
      "decision: reuse — Manifest step record matches current config hash and outputs exist.\n",
      "decision_meta: used_manifest=True forced=False\n",
      "notes:\n",
      " - Skipped compute based on reuse policy.\n",
      "outputs:\n",
      " - run_dir: runs\\RUN_SAMPLE_SMOKE_0001 (exists=True)\n",
      " - raw_login_attempt: runs\\RUN_SAMPLE_SMOKE_0001\\raw\\login_attempt.parquet (exists=True)\n",
      " - raw_checkout_attempt: runs\\RUN_SAMPLE_SMOKE_0001\\raw\\checkout_attempt.parquet (exists=True)\n",
      " - dataset_train: runs\\RUN_SAMPLE_SMOKE_0001\\dataset\\login_attempt\\train.parquet (exists=True)\n",
      " - dataset_time_eval: runs\\RUN_SAMPLE_SMOKE_0001\\dataset\\login_attempt\\time_eval.parquet (exists=True)\n",
      " - dataset_user_holdout: runs\\RUN_SAMPLE_SMOKE_0001\\dataset\\login_attempt\\user_holdout.parquet (exists=True)\n"
     ]
    }
   ],
   "source": [
    "from inkswarm_detectlab.ui.step_runner import step_dataset  # guard for out-of-order runs\n",
    "# Step 1 — Raw + Dataset (or reuse)\n",
    "rec = StepRecorder()\n",
    "out1 = None\n",
    "if DO_STEP_1_DATASET:\n",
    "    out1 = step_dataset(\n",
    "        cfg,\n",
    "        cfg_path=CFG_PATH,\n",
    "        run_id=run_id,\n",
    "        rec=rec,\n",
    "        reuse_if_exists=REUSE_IF_EXISTS,\n",
    "        force=FORCE_STEP_1_DATASET,\n",
    "    )\n",
    "    _print_outcome(out1)\n",
    "else:\n",
    "    print(\"Skipped (DO_STEP_1_DATASET=False)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c269a372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label definitions (synthetic scenarios):\n",
      "| label key | title | intent | how generated |\n",
      "|---|---|---|---|\n",
      "| `label_replicators` | Replicators | Automated or semi-automated high-volume login attempts (spray / scripted). | Assigned in the synthetic generator based on a per-row 'is_attacker' flag and an attack-campaign intensity bucket; deterministic given the generator seed. |\n",
      "| `label_the_mule` | The Mule | Credential-stuffing / account takeover style attempts with stronger success bias. | Assigned in the synthetic generator from the same attacker campaign scaffold; deterministic given the generator seed. |\n",
      "| `label_the_chameleon` | The Chameleon | Evasive attacker behavior designed to blend into normal traffic. | Assigned from the attacker campaign scaffold with higher 'stealth' characteristics; deterministic given the generator seed. |\n",
      "\n",
      "Train split size: 8142\n",
      "Label prevalence (train):\n",
      " - label_benign: 0.9442\n",
      " - label_replicators: 0.0257\n",
      " - label_the_chameleon: 0.0183\n",
      " - label_the_mule: 0.0119\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# Step 1b — Labels + sanity checks (lightweight)\n",
    "# Surfaces label assignment logic (non-random) and basic distributions.\n",
    "from inkswarm_detectlab.synthetic.label_defs import as_markdown_table\n",
    "from inkswarm_detectlab.io.tables import read_auto\n",
    "\n",
    "rdir = Path(cfg.paths.runs_dir) / run_id  # cfg.paths.runs_dir is already runs_dir\n",
    "\n",
    "print(\"\\nLabel definitions (synthetic scenarios):\")\n",
    "print(as_markdown_table())\n",
    "\n",
    "train_path = (rdir / \"dataset\" / \"login_attempt\" / \"train.parquet\")\n",
    "if train_path.exists():\n",
    "    df = read_auto(train_path)\n",
    "    label_cols = [c for c in df.columns if c.startswith(\"label_\")]\n",
    "    print(\"\\nTrain split size:\", len(df))\n",
    "    if label_cols:\n",
    "        print(\"Label prevalence (train):\")\n",
    "        for c in sorted(label_cols):\n",
    "            prev = float(df[c].mean())\n",
    "            print(f\" - {c}: {prev:.4f}\")\n",
    "    else:\n",
    "        print(\"No label_ columns found in dataset table (unexpected).\")\n",
    "else:\n",
    "    print(\"Train split missing; run Step 1 first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e2afa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP: features ===\n",
      "status: skipped\n",
      "decision: reuse — Manifest step record matches current config hash and outputs exist.\n",
      "decision_meta: used_manifest=True forced=False\n",
      "notes:\n",
      " - Skipped build based on reuse policy.\n",
      "outputs:\n",
      " - features_login: runs\\RUN_SAMPLE_SMOKE_0001\\features\\login_attempt\\features.parquet (exists=True)\n",
      "\n",
      "Log tail (featurelab):\n",
      "(no featurelab.log yet)\n"
     ]
    }
   ],
   "source": [
    "# Step 2 — Features (or reuse / shared-cache restore)\n",
    "out2 = None\n",
    "if DO_STEP_2_FEATURES:\n",
    "    out2 = step_features(\n",
    "        cfg,\n",
    "        cfg_path=CFG_PATH,\n",
    "        run_id=run_id,\n",
    "        rec=rec,\n",
    "        reuse_if_exists=REUSE_IF_EXISTS,\n",
    "        force=FORCE_STEP_2_FEATURES,\n",
    "        use_cache=USE_SHARED_FEATURE_CACHE,\n",
    "        write_cache=WRITE_SHARED_FEATURE_CACHE,\n",
    "    )\n",
    "    _print_outcome(out2)\n",
    "    print(\"\\nLog tail (featurelab):\")\n",
    "    log_path = Path(cfg.paths.runs_dir) / run_id / \"share\" / \"logs\" / \"featurelab.log\"\n",
    "    if log_path.exists():\n",
    "        print(tail_text(log_path, n_lines=120))\n",
    "    else:\n",
    "        print(\"(no featurelab.log yet)\")\n",
    "else:\n",
    "    print(\"Skipped (DO_STEP_2_FEATURES=False)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "584e52e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP: baselines ===\n",
      "status: skipped\n",
      "decision: reuse — Manifest step record matches current config hash and outputs exist.\n",
      "decision_meta: used_manifest=True forced=False\n",
      "notes:\n",
      " - Skipped training based on reuse policy.\n",
      "outputs:\n",
      " - baselines_dir: runs\\RUN_SAMPLE_SMOKE_0001\\models\\login_attempt\\baselines (exists=True)\n",
      " - metrics_json: runs\\RUN_SAMPLE_SMOKE_0001\\models\\login_attempt\\baselines\\metrics.json (exists=True)\n",
      "summary: {'env': {'pandas': '2.3.3', 'platform': 'Windows-10-10.0.19045-SP0', 'pyarrow': '22.0.0', 'python': '3.14.0 (tags/v3.14.0:ebf955d, Oct  7 2025, 10:15:03) [MSC v.1944 64 bit (AMD64)]', 'sklearn': '1.8.0', 'threadpools': [{'architecture': 'Haswell', 'filepath': 'C:\\\\Users\\\\Martín\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python314\\\\Lib\\\\site-packages\\\\numpy.libs\\\\libscipy_openblas64_-9e3e5a4229c1ca39f10dc82bba9e2b2b.dll', 'internal_api': 'openblas', 'num_threads': 8, 'prefix': 'libscipy_openblas', 'threading_layer': 'pthreads', 'user_api': 'blas', 'version': '0.3.30'}, {'filepath': 'C:\\\\Users\\\\Martín\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python314\\\\Lib\\\\site-packages\\\\sklearn\\\\.libs\\\\vcomp140.dll', 'internal_api': 'openmp', 'num_threads': 8, 'prefix': 'vcomp', 'user_api': 'openmp', 'version': None}, {'architecture': 'Haswell', 'filepath': 'C:\\\\Users\\\\Martín\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python314\\\\Lib\\\\site-packages\\\\scipy.libs\\\\libscipy_openblas-48c358d105077551cc9cc3ba79387ed5.dll', 'internal_api': 'openblas', 'num_threads': 8, 'prefix': 'libscipy_openblas', 'threading_layer': 'pthreads', 'user_api': 'blas', 'version': '0.3.29.dev'}]}, 'n_failed': 0, 'n_ok': 6, 'time_eval_rows': 1184, 'train_rows': 8142, 'user_holdout_rows': 1476}\n",
      "\n",
      "Baseline summary (meta):\n",
      "{\n",
      "  \"env\": {\n",
      "    \"pandas\": \"2.3.3\",\n",
      "    \"platform\": \"Windows-10-10.0.19045-SP0\",\n",
      "    \"pyarrow\": \"22.0.0\",\n",
      "    \"python\": \"3.14.0 (tags/v3.14.0:ebf955d, Oct  7 2025, 10:15:03) [MSC v.1944 64 bit (AMD64)]\",\n",
      "    \"sklearn\": \"1.8.0\",\n",
      "    \"threadpools\": [\n",
      "      {\n",
      "        \"architecture\": \"Haswell\",\n",
      "        \"filepath\": \"C:\\\\Users\\\\Mart\\u00edn\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python314\\\\Lib\\\\site-packages\\\\numpy.libs\\\\libscipy_openblas64_-9e3e5a4229c1ca39f10dc82bba9e2b2b.dll\",\n",
      "        \"internal_api\": \"openblas\",\n",
      "        \"num_threads\": 8,\n",
      "        \"prefix\": \"libscipy_openblas\",\n",
      "        \"threading_layer\": \"pthreads\",\n",
      "        \"user_api\": \"blas\",\n",
      "        \"version\": \"0.3.30\"\n",
      "      },\n",
      "      {\n",
      "        \"filepath\": \"C:\\\\Users\\\\Mart\\u00edn\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python314\\\\Lib\\\\site-packages\\\\sklearn\\\\.libs\\\\vcomp140.dll\",\n",
      "        \"internal_api\": \"openmp\",\n",
      "        \"num_threads\": 8,\n",
      "        \"prefix\": \"vcomp\",\n",
      "        \"user_api\": \"openmp\",\n",
      "        \"version\": null\n",
      "      },\n",
      "      {\n",
      "        \"architecture\": \"Haswell\",\n",
      "        \"filepath\": \"C:\\\\Users\\\\Mart\\u00edn\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python314\\\\Lib\\\\site-packages\\\\scipy.libs\\\\libscipy_openblas-48c358d105077551cc9cc3ba79387ed5.dll\",\n",
      "        \"internal_api\": \"openblas\",\n",
      "        \"num_threads\": 8,\n",
      "        \"prefix\": \"libscipy_openblas\",\n",
      "        \"threading_layer\": \"pthreads\",\n",
      "        \"user_api\": \"blas\",\n",
      "        \"version\": \"0.3.29.dev\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"n_failed\": 0,\n",
      "  \"n_ok\": 6,\n",
      "  \"time_eval_rows\": 1184,\n",
      "  \"train_rows\": 8142,\n",
      "  \"user_holdout_rows\": 1476\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 — Baselines (or reuse artifacts)\n",
    "out3 = None\n",
    "if DO_STEP_3_BASELINES:\n",
    "    out3 = step_baselines(\n",
    "        cfg,\n",
    "        run_id=run_id,\n",
    "        rec=rec,\n",
    "        reuse_if_exists=REUSE_IF_EXISTS,\n",
    "        force=FORCE_STEP_3_BASELINES,\n",
    "        cfg_path=CFG_PATH,\n",
    "    )\n",
    "    _print_outcome(out3)\n",
    "    # Quick look at metrics.json if present\n",
    "    metrics_path = Path(cfg.paths.runs_dir) / run_id / \"models\" / \"login_attempt\" / \"baselines\" / \"metrics.json\"\n",
    "    if metrics_path.exists():\n",
    "        metrics = json.loads(metrics_path.read_text(encoding=\"utf-8\"))\n",
    "        print(\"\\nBaseline summary (meta):\")\n",
    "        print(json.dumps(metrics.get(\"meta\", {}), indent=2))\n",
    "else:\n",
    "    print(\"Skipped (DO_STEP_3_BASELINES=False)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e4f8532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP: eval ===\n",
      "status: skipped\n",
      "decision: reuse — Manifest step record matches current config hash and outputs exist.\n",
      "decision_meta: used_manifest=True forced=False\n",
      "notes:\n",
      " - Skipped eval based on reuse policy.\n",
      "outputs:\n",
      " - reports_dir: runs\\RUN_SAMPLE_SMOKE_0001\\reports (exists=True)\n",
      " - eval_slices_json: runs\\RUN_SAMPLE_SMOKE_0001\\reports\\eval_slices_login_attempt.json (exists=True)\n",
      " - eval_stability_json: runs\\RUN_SAMPLE_SMOKE_0001\\reports\\eval_stability_login_attempt.json (exists=True)\n",
      " - eval_slices_md: runs\\RUN_SAMPLE_SMOKE_0001\\reports\\eval_slices_login_attempt.md (exists=True)\n",
      " - eval_stability_md: runs\\RUN_SAMPLE_SMOKE_0001\\reports\\eval_stability_login_attempt.md (exists=True)\n",
      " - reports/eval_slices_login_attempt.md: OK (runs\\RUN_SAMPLE_SMOKE_0001\\reports\\eval_slices_login_attempt.md)\n",
      " - reports/eval_stability_login_attempt.md: OK (runs\\RUN_SAMPLE_SMOKE_0001\\reports\\eval_stability_login_attempt.md)\n"
     ]
    }
   ],
   "source": [
    "# Step 4 — Eval (slice + stability reports)\n",
    "out4 = None\n",
    "if DO_STEP_4_EVAL:\n",
    "    out4 = step_eval(\n",
    "        cfg,\n",
    "        cfg_path=CFG_PATH,\n",
    "        run_id=run_id,\n",
    "        rec=rec,\n",
    "        reuse_if_exists=REUSE_IF_EXISTS,\n",
    "        force=FORCE_STEP_4_EVAL,\n",
    "    )\n",
    "    _print_outcome(out4)\n",
    "    # Point to key report files\n",
    "    rdir = Path(cfg.paths.runs_dir) / run_id\n",
    "    for rel in [\n",
    "        \"reports/eval_slices_login_attempt.md\",\n",
    "        \"reports/eval_stability_login_attempt.md\",\n",
    "    ]:\n",
    "        p = rdir / rel\n",
    "        print(f\" - {rel}: {'OK' if p.exists() else 'missing'} ({p})\")\n",
    "else:\n",
    "    print(\"Skipped (DO_STEP_4_EVAL=False)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f6e1362-c05f-4260-8084-f5a5cdd298b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export_ui_bundle signature: (cfg: 'AppConfig', *, run_ids: 'list[str]', out_dir: 'Path', force: 'bool' = False) -> 'Path'\n",
      "ui_bundle_dir: runs\\RUN_SAMPLE_SMOKE_0001\\share\\reports\n",
      "handover_path: runs\\RUN_SAMPLE_SMOKE_0001\\reports\\mvp_handover.md\n",
      "- share/logs: runs\\RUN_SAMPLE_SMOKE_0001\\share\\logs (missing)\n",
      "- share/reports: runs\\RUN_SAMPLE_SMOKE_0001\\share\\reports \n",
      "- models: runs\\RUN_SAMPLE_SMOKE_0001\\models \n",
      "- reports: runs\\RUN_SAMPLE_SMOKE_0001\\reports \n",
      "- logs: runs\\RUN_SAMPLE_SMOKE_0001\\logs \n",
      "- share/logs: runs\\RUN_SAMPLE_SMOKE_0001\\share\\logs (missing)\n",
      "- share/reports: runs\\RUN_SAMPLE_SMOKE_0001\\share\\reports \n",
      "- models: runs\\RUN_SAMPLE_SMOKE_0001\\models \n",
      "- reports: runs\\RUN_SAMPLE_SMOKE_0001\\reports \n",
      "- logs: runs\\RUN_SAMPLE_SMOKE_0001\\logs \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import inkswarm_detectlab.ui.step_runner as sr\n",
    "import inspect\n",
    "\n",
    "rdir = Path(cfg.paths.runs_dir) / run_id\n",
    "share_dir = rdir / \"share\"\n",
    "out_dir = share_dir / \"reports\"   # most consistent with the tree your notebook expects\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"export_ui_bundle signature:\", inspect.signature(sr.export_ui_bundle))\n",
    "\n",
    "sr.export_ui_bundle(cfg, run_ids=[run_id], out_dir=out_dir)\n",
    "from pathlib import Path\n",
    "import json\n",
    "import inkswarm_detectlab.ui.step_runner as sr\n",
    "\n",
    "runs_dir = Path(cfg.paths.runs_dir)\n",
    "rdir = runs_dir / run_id\n",
    "share_dir = rdir / \"share\"\n",
    "out_dir = share_dir / \"reports\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Write UI summary (so we have something to feed into handover)\n",
    "sr.write_ui_summary(cfg, run_id=run_id)\n",
    "\n",
    "# 2) Locate the summary artifact (try common locations)\n",
    "summary_path_candidates = [\n",
    "    share_dir / \"summary.json\",\n",
    "    share_dir / \"summary.md\",  # not json, but keep as fallback\n",
    "    rdir / \"reports\" / \"summary.json\",\n",
    "]\n",
    "summary = None\n",
    "\n",
    "for p in summary_path_candidates:\n",
    "    if p.exists() and p.suffix.lower() == \".json\":\n",
    "        summary = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "        print(\"Loaded summary from:\", p)\n",
    "        break\n",
    "\n",
    "if summary is None:\n",
    "    # fallback: minimal summary dict\n",
    "    summary = {\"run_id\": run_id, \"note\": \"summary.json not found; using minimal summary\"}\n",
    "\n",
    "# 3) Build UI bundle and capture where it went\n",
    "ui_bundle_dir = sr.export_ui_bundle(cfg, run_ids=[run_id], out_dir=out_dir)\n",
    "print(\"ui_bundle_dir:\", ui_bundle_dir)\n",
    "\n",
    "# 4) Exec summary (already working for you)\n",
    "sr.write_exec_summary(run_dir=rdir)\n",
    "\n",
    "# 5) Handover (NOW with required keyword-only args)\n",
    "handover_path = sr.write_mvp_handover(\n",
    "    runs_dir=runs_dir,\n",
    "    run_id=run_id,\n",
    "    ui_bundle_dir=Path(ui_bundle_dir) if ui_bundle_dir is not None else None,\n",
    "    summary=summary,\n",
    ")\n",
    "print(\"handover_path:\", handover_path)\n",
    "\n",
    "# 6) Tree\n",
    "from inkswarm_detectlab.ui.notebook_tools import print_run_tree\n",
    "print_run_tree(rdir)\n",
    "\n",
    "\n",
    "from inkswarm_detectlab.ui.notebook_tools import print_run_tree\n",
    "print_run_tree(rdir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ffa55ea-80a7-484e-813c-cdf1b63650f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write_mvp_handover signature: (*, runs_dir: 'Path', run_id: 'str', ui_bundle_dir: 'Path | None', summary: 'dict[str, Any]') -> 'Path'\n",
      "defined in: C:\\Users\\Martín\\Desktop\\inkswarm-core\\usul-inkswarm-detectlab\\src\\inkswarm_detectlab\\mvp\\handover.py\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(\"write_mvp_handover signature:\", inspect.signature(sr.write_mvp_handover))\n",
    "print(\"defined in:\", inspect.getsourcefile(sr.write_mvp_handover))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "402fc530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 paths:\n",
      " - runs_dir: runs\n",
      " - run_dir : runs\\RUN_SAMPLE_SMOKE_0001\n",
      " - share_dir: runs\\RUN_SAMPLE_SMOKE_0001\\share\n",
      " - ui_out_dir: runs\\RUN_SAMPLE_SMOKE_0001\\share\\reports\n",
      "\n",
      "[5A] write_ui_summary signature: (cfg: 'AppConfig', *, run_id: 'str', force: 'bool' = False) -> 'Path'\n",
      "[5A] summary keys: ['note', 'run_id'] \n",
      "\n",
      "[5B] write_exec_summary signature: (*, run_dir: 'Path', rr_provisional: 'bool' = True, d0004_deferred: 'bool' = True) -> 'ExecSummaryArtifacts'\n",
      "[5B] exec_summary artifacts: ExecSummaryArtifacts(exec_md=WindowsPath('runs/RUN_SAMPLE_SMOKE_0001/reports/EXEC_SUMMARY.md'), exec_html=WindowsPath('runs/RUN_SAMPLE_SMOKE_0001/reports/EXEC_SUMMARY.html'), summary_md=WindowsPath('runs/RUN_SAMPLE_SMOKE_0001/reports/summary.md'), summary_html=WindowsPath('runs/RUN_SAMPLE_SMOKE_0001/reports/summary.html'))\n",
      "\n",
      "[5C] export_ui_bundle signature: (cfg: 'AppConfig', *, run_ids: 'list[str]', out_dir: 'Path', force: 'bool' = False) -> 'Path'\n",
      "[5C] ui_bundle_dir: runs\\RUN_SAMPLE_SMOKE_0001\\share\\reports\n",
      "\n",
      "[5D] write_mvp_handover signature: (*, runs_dir: 'Path', run_id: 'str', ui_bundle_dir: 'Path | None', summary: 'dict[str, Any]') -> 'Path'\n",
      "[5D] handover_path: runs\\RUN_SAMPLE_SMOKE_0001\\reports\\mvp_handover.md\n",
      "\n",
      "[5E] evidence bundle (optional)\n",
      "[5E] evidence bundle skipped: export_evidence_bundle() got an unexpected keyword argument 'run_id'. Did you mean 'run_dir'?\n",
      "\n",
      "Run tree (after Step 5 manual export):\n",
      "- share/logs: runs\\RUN_SAMPLE_SMOKE_0001\\share\\logs (missing)\n",
      "- share/reports: runs\\RUN_SAMPLE_SMOKE_0001\\share\\reports \n",
      "- models: runs\\RUN_SAMPLE_SMOKE_0001\\models \n",
      "- reports: runs\\RUN_SAMPLE_SMOKE_0001\\reports \n",
      "- logs: runs\\RUN_SAMPLE_SMOKE_0001\\logs \n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Step 5 — Export share bundle (manual, robust)\n",
    "# ==========================\n",
    "# Goals:\n",
    "# - Avoid patches by calling the current functions with their actual signatures.\n",
    "# - Produce: UI summary, exec summary, UI bundle, handover, (optional) evidence bundle.\n",
    "# - Be restart-safe and print where outputs landed.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import inspect\n",
    "\n",
    "import inkswarm_detectlab.ui.step_runner as sr\n",
    "from inkswarm_detectlab.ui.notebook_tools import print_run_tree\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 0) Resolve key paths\n",
    "# --------------------------\n",
    "runs_dir = Path(cfg.paths.runs_dir)\n",
    "rdir = runs_dir / run_id\n",
    "share_dir = rdir / \"share\"\n",
    "share_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Where to write bundle assets (your export_ui_bundle requires out_dir)\n",
    "ui_out_dir = share_dir / \"reports\"\n",
    "ui_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Step 5 paths:\")\n",
    "print(\" - runs_dir:\", runs_dir)\n",
    "print(\" - run_dir :\", rdir)\n",
    "print(\" - share_dir:\", share_dir)\n",
    "print(\" - ui_out_dir:\", ui_out_dir)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 1) UI Summary\n",
    "# --------------------------\n",
    "print(\"\\n[5A] write_ui_summary signature:\", inspect.signature(sr.write_ui_summary))\n",
    "sr.write_ui_summary(cfg, run_id=run_id)\n",
    "\n",
    "# Try to load a JSON summary if it exists (handover needs a dict summary)\n",
    "summary_candidates = [\n",
    "    share_dir / \"summary.json\",\n",
    "    share_dir / \"ui_summary.json\",\n",
    "    rdir / \"reports\" / \"summary.json\",\n",
    "    rdir / \"share\" / \"summary.json\",\n",
    "]\n",
    "\n",
    "summary: dict = {\"run_id\": run_id, \"note\": \"summary.json not found; using minimal summary dict\"}\n",
    "for p in summary_candidates:\n",
    "    if p.exists() and p.suffix.lower() == \".json\":\n",
    "        try:\n",
    "            summary = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "            print(\"[5A] Loaded summary JSON:\", p)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"[5A] Failed reading summary JSON:\", p, \"err=\", e)\n",
    "\n",
    "print(\"[5A] summary keys:\", sorted(list(summary.keys()))[:25], (\"...\" if len(summary.keys()) > 25 else \"\"))\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 2) Exec Summary (keyword-only)\n",
    "# --------------------------\n",
    "print(\"\\n[5B] write_exec_summary signature:\", inspect.signature(sr.write_exec_summary))\n",
    "# Current signature (confirmed): (*, run_dir: Path, rr_provisional=True, d0004_deferred=True)\n",
    "exec_art = sr.write_exec_summary(run_dir=rdir)\n",
    "print(\"[5B] exec_summary artifacts:\", exec_art)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3) UI Bundle (keyword-only run_ids + out_dir)\n",
    "# --------------------------\n",
    "print(\"\\n[5C] export_ui_bundle signature:\", inspect.signature(sr.export_ui_bundle))\n",
    "# Current signature (confirmed): (cfg, *, run_ids: list[str], out_dir: Path, force=False) -> Path\n",
    "ui_bundle_dir = sr.export_ui_bundle(cfg, run_ids=[run_id], out_dir=ui_out_dir)\n",
    "print(\"[5C] ui_bundle_dir:\", ui_bundle_dir)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 4) Handover (keyword-only, requires runs_dir + ui_bundle_dir + summary dict)\n",
    "# --------------------------\n",
    "print(\"\\n[5D] write_mvp_handover signature:\", inspect.signature(sr.write_mvp_handover))\n",
    "# Current signature (confirmed):\n",
    "#   (*, runs_dir: Path, run_id: str, ui_bundle_dir: Path|None, summary: dict[str, Any]) -> Path\n",
    "handover_path = sr.write_mvp_handover(\n",
    "    runs_dir=runs_dir,\n",
    "    run_id=run_id,\n",
    "    ui_bundle_dir=Path(ui_bundle_dir) if ui_bundle_dir is not None else None,\n",
    "    summary=summary,\n",
    ")\n",
    "print(\"[5D] handover_path:\", handover_path)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 5) Evidence bundle (optional)\n",
    "# --------------------------\n",
    "print(\"\\n[5E] evidence bundle (optional)\")\n",
    "try:\n",
    "    from inkswarm_detectlab.share.evidence import export_evidence_bundle\n",
    "    export_evidence_bundle(cfg, run_id=run_id)\n",
    "    print(\"[5E] evidence bundle: OK\")\n",
    "except Exception as e:\n",
    "    print(\"[5E] evidence bundle skipped:\", e)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 6) Final tree\n",
    "# --------------------------\n",
    "print(\"\\nRun tree (after Step 5 manual export):\")\n",
    "print_run_tree(rdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cec9b34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Step summary\n",
      "\n",
      "(no steps recorded)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step summary table (D-0022 visibility)\n",
    "print(rec.to_markdown())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7e6df-5963-4da1-827a-eab9e86631b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
